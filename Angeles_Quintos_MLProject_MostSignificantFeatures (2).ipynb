{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project (Text-Based Gender Classification of Twitter Data using Naive Bayes and Support Vector Machine)\n",
    "# Feature Extraction using the 10 Most Significant Meta-Attributes\n",
    "## Quintos, Maria Nikki H.\n",
    "## Angeles, Angelic L.\n",
    "## BSCS-ML COM 181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.   Importing the modules, libraries, and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A library is essentially a collection of modules that can be called and used and we can import the libraries by using 'import' to make use of the different functions. This part of the code imports modules and libraries to have access to the functions that we will be needing to build the classification model using Naive Bayes and SVM. Python libraries such as:\n",
    "\n",
    "1. <b>Numpy</b> - works with arrays and matrices used for computation in the algorithm \n",
    "2. <b>Pandas</b> - works with data manipulation and analysis \n",
    "3. <b>Sklearn</b> - works with classification, clustering and regression\n",
    "4. <b>Re</b> - works with regular expressions which are a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing the dataset</b>\n",
    "\n",
    "Data is raw information, it is the representation of both human and machine observation of the world. The dataset that you should used entirely depends on what type of problem you want to solve. This study is mainly focus on the gender classification problem and in this part of the code we are importing the dataset that we will used in the classification task. The dataset collected is from Kaggle. The dataset has 26 independent variables and 20,050 data. Since we are focusing on the text-based gender classification, we will only get the “gender” and “text” variables, where in the “text” represents the Twitter user’s tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20050, 26)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/rahulvks/gender-identification-analysis/notebook\n",
    "# https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/\n",
    "\n",
    "url_train = 'https://raw.githubusercontent.com/mdeff/ntds_2016/master/project/reports/twitter_gender/gender-classifier-DFE-791531.csv'\n",
    "data_with_duplicate = pd.read_csv(url_train, encoding='latin1')\n",
    "\n",
    "data_with_duplicate.head(5)\n",
    "data_with_duplicate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are living in the age of data and data are very significant when implementing machine learning algorithms since this is where the model gets all the information that they should process and learn. Data Pre-Processing is a technique to prepare the data to get more out of it. To have better and improved datasets that are cleaner and are more manageable to analyze and work with, we must apply this technique. The purpose of using this technique is to avoid noisy, inconsistent, and missing data that can lead to poor accuracy of the machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dropping the duplicates</b>\n",
    "\n",
    "In this part of the code we are simply removing the duplicates from the DataFrame using the 'drop_duplicates' method and storing the new DataFrame with duplicate rows removed in the 'data' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20050\n",
      "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
      "       '_last_judgment_at', 'gender', 'gender:confidence', 'profile_yn',\n",
      "       'profile_yn:confidence', 'created', 'description', 'fav_number',\n",
      "       'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage',\n",
      "       'retweet_count', 'sidebar_color', 'text', 'tweet_coord', 'tweet_count',\n",
      "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "18412\n"
     ]
    }
   ],
   "source": [
    "print(len(data_with_duplicate))\n",
    "print(data_with_duplicate.columns)\n",
    "#print(data_with_duplicate.shape)\n",
    "data = data_with_duplicate.sort_values('text', ascending=True).drop_duplicates('text').sort_index()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Getting the significant columns that we will used in the classification task and dealing with missing values</b>\n",
    "\n",
    "The dataset will surely have missing and noisy data because the data gathering process is not perfect, so the dataset will have many irrelevant and missing parts. Data cleaning is one of the method that we should use to solve this problem. In this part of the code, we are only getting the significant columns that we will used in the classification task which are 'text' (independent variable) that represents the Twitter user's tweets and 'gender' (dependent variable) that represents if the Twitter user is a male or female. Then after getting the significant columns, we will find all the null values in the dataset and replace it with 'XXX' just so that we can easily distinguish if there is any null or missing values in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  gender\n",
      "0  Robbie E Responds To Critics After Win Against...    male\n",
      "1  ÛÏIt felt like they were my friends and I was...    male\n",
      "2  i absolutely adore when louis starts the songs...    male\n",
      "3  Hi @JordanSpieth - Looking at the url - do you...    male\n",
      "4  Watching Neighbours on Sky+ catching up with t...  female\n",
      "cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-52e004f9ebe3>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text_label['gender'] = data_text_label['gender'].fillna('XXX')\n"
     ]
    }
   ],
   "source": [
    "# only get the text and label.\n",
    "data_text_label = data[['text', 'gender']]\n",
    "print(data_text_label.head(5))\n",
    "\n",
    "# sanity check of our data\n",
    "if data_text_label['gender'].isnull().any():\n",
    "  data_text_label['gender'] = data_text_label['gender'].fillna('XXX')\n",
    "  print('cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Cleaning</b>\n",
    "\n",
    "One of the data preprocessing techniques is data cleaning, by applying this technique the model can improve the accuracy and performance of the machine learning model. The Regular Expression (RegEx) method is a faster process of cleaning text data that is simpler to use compared to manually splitting the strings. Regular Expressions (RegEx) are essentially text patterns that you can use to automate searching through and replacing elements within strings of text. We convert all strings into lowercase using '.lower()' method and in this case, we used 're.sub()' method to replace irrelevant and insignificant strings or characters in the 'text' or tweets of the users such as:\n",
    "1. Converting all strings into lower case\n",
    "2. Links and urls are removed\n",
    "3. Remove all non-ASCII code\n",
    "4. Remove all punctuations\n",
    "5. Delete double spaces and replace with one space\n",
    "6. Remove special characters\n",
    "7. Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a787638b0983>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text_label['text'] = data_text_label.apply(lambda x: cleaning(x['text']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    encoded_string = s.encode(\"ascii\", \"ignore\")\n",
    "    s = encoded_string.decode()\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    s = re.sub(r\"\\b\\d+\\b\", \"\", s)\n",
    "    s = re.sub(r'[0-9]+', '', s)\n",
    "    return s\n",
    "'''\n",
    "def sample(s):\n",
    "  return 'M'+s\n",
    "'''\n",
    "\n",
    "data_text_label['text'] = data_text_label.apply(lambda x: cleaning(x['text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extraction of 10 Most Significant Features or Meta-Attributes</b>\n",
    "\n",
    "In this part of the code, this is where we create all the methods or functions that we will used to extract the 10 most significant features or create the 10 most significant meta-attributes. We extracted 10 most significant features and meta-attributes. These methods are the following:\n",
    "1. <b>count_word_in_sentence</b> - counts the number of words in a sentence.\n",
    "2. <b>count_char_in_sentence</b> - counts the number of characters in a sentence.\n",
    "3. <b>aveNum_of_char_per_word</b> - is the ratio of number of characters and number of words.\n",
    "4. <b>count_spaces</b> - counts the number of blank spaces in a text. \n",
    "5. <b>ratioSpacesAndCharacters</b> - is the ratio of number of spaces and number of characters.\n",
    "6. <b>countWords</b> - counts the total number of words with more than 6 characters in a sentence.\n",
    "7. <b>ratio6wordsOvertotalWord</b> - is the ratio of the number of words with more than 6 characters and the total number of words.\n",
    "8. <b>ratioPronounsAndWords</b> - is the ratio of number of pronouns and number of words.\n",
    "9. <b>ratioAuxiliaryVerbsAndWords</b> - is the ratio of number of auxiliary verbs and number of words.\n",
    "10. <b>ratioConjunctionsAndWords</b> - is the ratio of number of conjunctions and number of words.\n",
    "11. <b>ratioInterjectionsAndWords</b> - is the ratio of number of interjections and number of words.\n",
    "12. <b>ratioPrepositionsAndWords</b> - is the ratio of number of prepositions and number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_sentence(x):\n",
    "    word_list = x.split(' ')\n",
    "    return len(word_list)\n",
    "\n",
    "\n",
    "def count_char_in_sentence(char):\n",
    "    count_char = len(char) - char.count(\" \")\n",
    "    return (count_char)\n",
    "\n",
    "\n",
    "def aveNum_of_char_per_word(a):\n",
    "    word = a.split(' ')\n",
    "    word_split = len(word)\n",
    "    count_char = len(a) - a.count(\" \")\n",
    "    return (word_split/count_char)\n",
    "\n",
    "\n",
    "\n",
    "def count_spaces(s):\n",
    "    return sum(1 for x in s if x.isspace())\n",
    "\n",
    "\n",
    "def ratioSpacesAndCharacters(a):\n",
    "    spaces = sum(1 for x in a if x.isspace())\n",
    "    char = len(a) - a.count(\" \")\n",
    "    ratio = (spaces/char)\n",
    "    return (ratio)\n",
    "\n",
    "\n",
    "def countWords(s):\n",
    "    words = s.split()\n",
    "    return len([word for word in words if len(word)>=6])\n",
    "\n",
    "def ratio6wordsOvertotalWord(b):\n",
    "    words = b.split(\" \")\n",
    "    count = len([word for word in words if len(word)>=6])\n",
    "    length = len(words)\n",
    "    ratio = (count/length)\n",
    "    return (ratio)\n",
    "\n",
    "\n",
    "def ratioPronounsAndWords(a):\n",
    "    wordlist = a.split(' ')\n",
    "    pronouns = [\"I\", \"i\", \"Me\", \"me\", \"You\", \"you\", \"He\", \"he\", \"Him\", \"him\", \"She\", \"she\", \"Her\", \"her\", \"It\", \"it\", \"We\", \"we\", \"Us\", \"us\", \"They\", \"they\", \"Them\", \"them\"]\n",
    "    num_pronouns = 0\n",
    "    for i in wordlist:\n",
    "        if i in pronouns: \n",
    "            num_pronouns += 1\n",
    "    \n",
    "    num_words = a.split(' ')\n",
    "    num_words = len(num_words)\n",
    "    ratio = (num_pronouns/num_words)\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def ratioAuxiliaryVerbsAndWords(a):\n",
    "    wordlist = a.split(' ')\n",
    "    auxiliary_verbs = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"being\", \"can\", \"could\", \"do\", \"did\", \"does\", \"doing\", \"have\", \"had\", \"has\", \"having\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\",\"Be\", \"Am\", \"Are\", \"Is\", \"Was\", \"Were\", \"Being\", \"Can\", \"Could\", \"Do\", \"Did\", \"Does\", \"Doing\", \"Have\", \"Had\", \"Has\", \"Having\", \"May\", \"Might\", \"Must\", \"Shall\", \"Should\", \"Will\", \"Would\"]\n",
    "    num_auxiliary_verbs = 0\n",
    "    for i in wordlist:\n",
    "        if i in auxiliary_verbs:\n",
    "            num_auxiliary_verbs += 1\n",
    "    \n",
    "    num_words = a.split(' ')\n",
    "    num_words = len(num_words)\n",
    "    ratio = (num_auxiliary_verbs/num_words)\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def ratioConjunctionsAndWords(a):\n",
    "    wordlist = a.split(' ')\n",
    "    conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"yet\", \"And\", \"But\", \"For\", \"Nor\", \"Or\", \"So\", \"Yet\", \"after\", \"until\", \"before\", \"since\", \"because\", \"as\", \"though\", \"although\", \"where as\", \"while\", \"After\", \"Until\", \"Before\", \"Since\", \"Because\", \"As\", \"Though\", \"Although\", \"Where as\", \"While\",\"either\", \"neither\", \"not only\", \"but also\", \"both\", \"not\", \"whether\", \"just as\", \" as\", \"as much\", \"no sooner\", \"than\", \"rather\", \"Either\", \"Neither\", \"Not only\", \"But also\", \"Both\", \"Not\", \"Whether\", \"Just as\", \" As\", \"As much\", \"No sooner\", \"Than\", \"Rather\"]\n",
    "    num_conjunctions = 0\n",
    "    for i in wordlist:\n",
    "        if i in conjunctions:\n",
    "            num_conjunctions += 1\n",
    "    \n",
    "    num_words = a.split(' ')\n",
    "    num_words = len(num_words)\n",
    "    ratio = (num_conjunctions/num_words)\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def ratioInterjectionsAndWords(a):\n",
    "    wordlist = a.split(' ')\n",
    "    interjections = [\"ah\", \"alas\", \"dear\", \"eh\", \"er\", \"god\", \"hello\", \"hey\", \"hi\", \"hmm\", \"oh\", \"o\", \"ok\", \"okay\", \"ouch\", \"uh\", \"uh-huh\", \"um\", \"umm\", \"well\", \"wow\", \"Ah\", \"Alas\", \"Dear\", \"Eh\", \"Er\", \"God\", \"Hello\", \"Hey\", \"Hi\", \"Hmm\", \"Oh\", \"O\", \"Ok\", \"Okay\", \"Ouch\", \"Uh\", \"Uh-huh\", \"Um\", \"Umm\", \"Well\", \"Wow\"]\n",
    "    num_interjections = 0\n",
    "    for i in wordlist:\n",
    "        if i in interjections:\n",
    "            num_interjections += 1\n",
    "    \n",
    "    num_words = a.split(' ')\n",
    "    num_words = len(num_words)\n",
    "    ratio = (num_interjections/num_words)\n",
    "    return ratio\n",
    "\n",
    "def ratioPrepositionsAndWords(a):\n",
    "    wordlist = a.split(' ')\n",
    "    prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"anti\", \"around\", \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"besides\", \"between\", \"beyond\", \"but\", \"by\", \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"excepting\", \"excluding\", \"following\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"minus\", \"near\", \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"outside\", \"over\", \"past\", \"per\", \"plus\", \"regarding\", \"round\", \"save\", \"since\", \"than\", \"through\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\", \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\"]\n",
    "    num_prepositions = 0\n",
    "    for i in wordlist:\n",
    "        if i in prepositions:\n",
    "            num_prepositions += 1\n",
    "    \n",
    "    num_words = a.split(' ')\n",
    "    num_words = len(num_words)\n",
    "    ratio = (num_prepositions/num_words)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application of Created Methods for Feature Extraction or Creation of 10 Most Significant Meta-Attributes in the DataFrame</b>\n",
    "\n",
    "We created a dataframe named 'duplicate' and this is where we will store our extracted features or meta-attributes. First, we insert the dataset to the dataframe and store it in the 'duplicate' variable then we apply all the 12 methods that we created from the previous step for feature extraction. Then store the value of all the result of these methods in the 'duplicate' dataframe. The dataset has now 12 columns from originally having 2 columns only which is the 'text' which represents the tweets and 'gender' columns. But now we extracted 10 most significant (independent variables) features or meta-attributes which are LengthOfWord, CountChar, AveCharWord, RationSpaChar, Ratio6WordsTotalWord, ProWords,\tAuxiliaryVerbsAndWords, ConjunctionsAndWords, InterjectionsAndWords, and PrepositionsAndWords. Including the 'text' column which represents the user's tweets. Then, 'gender' as the dependent variable. Having a total of 12 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>LengthOfWord</th>\n",
       "      <th>CountChar</th>\n",
       "      <th>AveCharWord</th>\n",
       "      <th>RationSpaChar</th>\n",
       "      <th>Ratio6WordsTotalWord</th>\n",
       "      <th>ProWords</th>\n",
       "      <th>AuxiliaryVerbsAndWords</th>\n",
       "      <th>ConjunctionsAndWords</th>\n",
       "      <th>InterjectionsAndWords</th>\n",
       "      <th>PrepositionsAndWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robbie e responds to critics after win against...</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>72</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it felt like they were my friends and i was li...</td>\n",
       "      <td>male</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>65</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi jordanspieth looking at the url do you use ...</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>84</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watching neighbours on sky catching up with th...</td>\n",
       "      <td>female</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  gender  LengthOfWord  \\\n",
       "0  robbie e responds to critics after win against...    male            14   \n",
       "1  it felt like they were my friends and i was li...    male            18   \n",
       "2  i absolutely adore when louis starts the songs...    male            16   \n",
       "3  hi jordanspieth looking at the url do you use ...    male            21   \n",
       "4  watching neighbours on sky catching up with th...  female            11   \n",
       "\n",
       "   CountChar  AveCharWord  RationSpaChar  Ratio6WordsTotalWord  ProWords  \\\n",
       "0         72     0.194444       0.180556              0.428571  0.000000   \n",
       "1         66     0.272727       0.257576              0.166667  0.222222   \n",
       "2         65     0.246154       0.230769              0.125000  0.250000   \n",
       "3         84     0.250000       0.238095              0.238095  0.047619   \n",
       "4         53     0.207547       0.188679              0.363636  0.000000   \n",
       "\n",
       "   AuxiliaryVerbsAndWords  ConjunctionsAndWords  InterjectionsAndWords  \\\n",
       "0                0.000000              0.071429               0.000000   \n",
       "1                0.111111              0.055556               0.000000   \n",
       "2                0.000000              0.062500               0.000000   \n",
       "3                0.047619              0.000000               0.047619   \n",
       "4                0.000000              0.000000               0.000000   \n",
       "\n",
       "   PrepositionsAndWords  \n",
       "0              0.285714  \n",
       "1              0.111111  \n",
       "2              0.062500  \n",
       "3              0.095238  \n",
       "4              0.272727  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(count_word_in_sentence)\n",
    "duplicate['LengthOfWord'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(count_char_in_sentence)\n",
    "duplicate['CountChar'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(aveNum_of_char_per_word)\n",
    "duplicate['AveCharWord'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioSpacesAndCharacters)\n",
    "duplicate['RationSpaChar'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratio6wordsOvertotalWord)\n",
    "duplicate['Ratio6WordsTotalWord'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioPronounsAndWords)\n",
    "duplicate['ProWords'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioAuxiliaryVerbsAndWords)\n",
    "duplicate['AuxiliaryVerbsAndWords'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioConjunctionsAndWords)\n",
    "duplicate['ConjunctionsAndWords'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioInterjectionsAndWords)\n",
    "duplicate['InterjectionsAndWords'] = g\n",
    "duplicate.head(5)\n",
    "\n",
    "\n",
    "\n",
    "duplicate = pd.DataFrame(data_text_label)\n",
    "g = data_text_label['text'].apply(ratioPrepositionsAndWords)\n",
    "duplicate['PrepositionsAndWords'] = g\n",
    "duplicate.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>LengthOfWord</th>\n",
       "      <th>CountChar</th>\n",
       "      <th>AveCharWord</th>\n",
       "      <th>RationSpaChar</th>\n",
       "      <th>Ratio6WordsTotalWord</th>\n",
       "      <th>ProWords</th>\n",
       "      <th>AuxiliaryVerbsAndWords</th>\n",
       "      <th>ConjunctionsAndWords</th>\n",
       "      <th>InterjectionsAndWords</th>\n",
       "      <th>PrepositionsAndWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robbie e responds to critics after win against...</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>72</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it felt like they were my friends and i was li...</td>\n",
       "      <td>male</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>65</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi jordanspieth looking at the url do you use ...</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>84</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watching neighbours on sky catching up with th...</td>\n",
       "      <td>female</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20032</th>\n",
       "      <td>legobutts you can do quests and kill stuff wh...</td>\n",
       "      <td>male</td>\n",
       "      <td>29</td>\n",
       "      <td>106</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20034</th>\n",
       "      <td>view the community halloween spooktacular city...</td>\n",
       "      <td>brand</td>\n",
       "      <td>9</td>\n",
       "      <td>62</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20036</th>\n",
       "      <td>itsleehinchy leesqanda what s the story in ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20042</th>\n",
       "      <td>best bloody mary brunch at the nycwff nyceff d...</td>\n",
       "      <td>brand</td>\n",
       "      <td>12</td>\n",
       "      <td>73</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>lookupondeath fine and i ll drink tea too i l...</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18412 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  gender  \\\n",
       "0      robbie e responds to critics after win against...    male   \n",
       "1      it felt like they were my friends and i was li...    male   \n",
       "2      i absolutely adore when louis starts the songs...    male   \n",
       "3      hi jordanspieth looking at the url do you use ...    male   \n",
       "4      watching neighbours on sky catching up with th...  female   \n",
       "...                                                  ...     ...   \n",
       "20032   legobutts you can do quests and kill stuff wh...    male   \n",
       "20034  view the community halloween spooktacular city...   brand   \n",
       "20036   itsleehinchy leesqanda what s the story in ba...  female   \n",
       "20042  best bloody mary brunch at the nycwff nyceff d...   brand   \n",
       "20045   lookupondeath fine and i ll drink tea too i l...  female   \n",
       "\n",
       "       LengthOfWord  CountChar  AveCharWord  RationSpaChar  \\\n",
       "0                14         72     0.194444       0.180556   \n",
       "1                18         66     0.272727       0.257576   \n",
       "2                16         65     0.246154       0.230769   \n",
       "3                21         84     0.250000       0.238095   \n",
       "4                11         53     0.207547       0.188679   \n",
       "...             ...        ...          ...            ...   \n",
       "20032            29        106     0.273585       0.264151   \n",
       "20034             9         62     0.145161       0.129032   \n",
       "20036            10         44     0.227273       0.204545   \n",
       "20042            12         73     0.164384       0.150685   \n",
       "20045            13         42     0.309524       0.285714   \n",
       "\n",
       "       Ratio6WordsTotalWord  ProWords  AuxiliaryVerbsAndWords  \\\n",
       "0                  0.428571  0.000000                0.000000   \n",
       "1                  0.166667  0.222222                0.111111   \n",
       "2                  0.125000  0.250000                0.000000   \n",
       "3                  0.238095  0.047619                0.047619   \n",
       "4                  0.363636  0.000000                0.000000   \n",
       "...                     ...       ...                     ...   \n",
       "20032              0.172414  0.137931                0.103448   \n",
       "20034              0.444444  0.000000                0.000000   \n",
       "20036              0.300000  0.000000                0.000000   \n",
       "20042              0.583333  0.000000                0.000000   \n",
       "20045              0.076923  0.230769                0.000000   \n",
       "\n",
       "       ConjunctionsAndWords  InterjectionsAndWords  PrepositionsAndWords  \n",
       "0                  0.071429               0.000000              0.285714  \n",
       "1                  0.055556               0.000000              0.111111  \n",
       "2                  0.062500               0.000000              0.062500  \n",
       "3                  0.000000               0.047619              0.095238  \n",
       "4                  0.000000               0.000000              0.272727  \n",
       "...                     ...                    ...                   ...  \n",
       "20032              0.103448               0.000000              0.068966  \n",
       "20034              0.000000               0.000000              0.000000  \n",
       "20036              0.000000               0.000000              0.100000  \n",
       "20042              0.000000               0.000000              0.083333  \n",
       "20045              0.076923               0.000000              0.000000  \n",
       "\n",
       "[18412 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removal of text column</b>\n",
    "\n",
    "In this part of the code, we will now remove the 'text' column since we will not use it anymore and will no longer be significant in the dataset because we already created 10 most significant features or meta-attributes that is extracted from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>LengthOfWord</th>\n",
       "      <th>CountChar</th>\n",
       "      <th>AveCharWord</th>\n",
       "      <th>RationSpaChar</th>\n",
       "      <th>Ratio6WordsTotalWord</th>\n",
       "      <th>ProWords</th>\n",
       "      <th>AuxiliaryVerbsAndWords</th>\n",
       "      <th>ConjunctionsAndWords</th>\n",
       "      <th>InterjectionsAndWords</th>\n",
       "      <th>PrepositionsAndWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>72</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>65</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>84</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  LengthOfWord  CountChar  AveCharWord  RationSpaChar  \\\n",
       "0    male            14         72     0.194444       0.180556   \n",
       "1    male            18         66     0.272727       0.257576   \n",
       "2    male            16         65     0.246154       0.230769   \n",
       "3    male            21         84     0.250000       0.238095   \n",
       "4  female            11         53     0.207547       0.188679   \n",
       "\n",
       "   Ratio6WordsTotalWord  ProWords  AuxiliaryVerbsAndWords  \\\n",
       "0              0.428571  0.000000                0.000000   \n",
       "1              0.166667  0.222222                0.111111   \n",
       "2              0.125000  0.250000                0.000000   \n",
       "3              0.238095  0.047619                0.047619   \n",
       "4              0.363636  0.000000                0.000000   \n",
       "\n",
       "   ConjunctionsAndWords  InterjectionsAndWords  PrepositionsAndWords  \n",
       "0              0.071429               0.000000              0.285714  \n",
       "1              0.055556               0.000000              0.111111  \n",
       "2              0.062500               0.000000              0.062500  \n",
       "3              0.000000               0.047619              0.095238  \n",
       "4              0.000000               0.000000              0.272727  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = pd.DataFrame(data_text_label)\n",
    "del duplicate['text']\n",
    "duplicate.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Shifting the gender column to the rightmost part of the data</b>\n",
    "\n",
    "We now only have 11 columns since we already remove the text column and we shifted the gender column to the rightmost part of the dataframe for a cleaner look in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_shift = ['gender']\n",
    "duplicate = pd.concat([duplicate[duplicate.columns.difference(column_to_shift)], duplicate[column_to_shift]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuxiliaryVerbsAndWords</th>\n",
       "      <th>AveCharWord</th>\n",
       "      <th>ConjunctionsAndWords</th>\n",
       "      <th>CountChar</th>\n",
       "      <th>InterjectionsAndWords</th>\n",
       "      <th>LengthOfWord</th>\n",
       "      <th>PrepositionsAndWords</th>\n",
       "      <th>ProWords</th>\n",
       "      <th>Ratio6WordsTotalWord</th>\n",
       "      <th>RationSpaChar</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20032</th>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20034</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20036</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20042</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18412 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AuxiliaryVerbsAndWords  AveCharWord  ConjunctionsAndWords  CountChar  \\\n",
       "0                    0.000000     0.194444              0.071429         72   \n",
       "1                    0.111111     0.272727              0.055556         66   \n",
       "2                    0.000000     0.246154              0.062500         65   \n",
       "3                    0.047619     0.250000              0.000000         84   \n",
       "4                    0.000000     0.207547              0.000000         53   \n",
       "...                       ...          ...                   ...        ...   \n",
       "20032                0.103448     0.273585              0.103448        106   \n",
       "20034                0.000000     0.145161              0.000000         62   \n",
       "20036                0.000000     0.227273              0.000000         44   \n",
       "20042                0.000000     0.164384              0.000000         73   \n",
       "20045                0.000000     0.309524              0.076923         42   \n",
       "\n",
       "       InterjectionsAndWords  LengthOfWord  PrepositionsAndWords  ProWords  \\\n",
       "0                   0.000000            14              0.285714  0.000000   \n",
       "1                   0.000000            18              0.111111  0.222222   \n",
       "2                   0.000000            16              0.062500  0.250000   \n",
       "3                   0.047619            21              0.095238  0.047619   \n",
       "4                   0.000000            11              0.272727  0.000000   \n",
       "...                      ...           ...                   ...       ...   \n",
       "20032               0.000000            29              0.068966  0.137931   \n",
       "20034               0.000000             9              0.000000  0.000000   \n",
       "20036               0.000000            10              0.100000  0.000000   \n",
       "20042               0.000000            12              0.083333  0.000000   \n",
       "20045               0.000000            13              0.000000  0.230769   \n",
       "\n",
       "       Ratio6WordsTotalWord  RationSpaChar  gender  \n",
       "0                  0.428571       0.180556    male  \n",
       "1                  0.166667       0.257576    male  \n",
       "2                  0.125000       0.230769    male  \n",
       "3                  0.238095       0.238095    male  \n",
       "4                  0.363636       0.188679  female  \n",
       "...                     ...            ...     ...  \n",
       "20032              0.172414       0.264151    male  \n",
       "20034              0.444444       0.129032   brand  \n",
       "20036              0.300000       0.204545  female  \n",
       "20042              0.583333       0.150685   brand  \n",
       "20045              0.076923       0.285714  female  \n",
       "\n",
       "[18412 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Storing the 'duplicate' dataframe that we created where in there are 10 most significant features extracted already to the main variable of the data which is 'data_text_label'</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_label = duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analyzing the classes on the dependent variable</b>\n",
    "\n",
    "In this code, we are getting the number of records or occurences in the 'gender' per class. As of now, there are 5 classes in the dependent variable which are female, male, brand or typically a type of product or business manufactured by a particular company under a particular name, unknown or users that are not identified, and XXX refers to the null or missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6573\n",
       "male       6136\n",
       "brand      4729\n",
       "unknown     877\n",
       "XXX          97\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_label['gender'].value_counts()\n",
    "#data_text_label['gender'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removal of insignificant classes in 'gender' and checking if there are still missing values</b>\n",
    "\n",
    "We remove the brand, unknown, and the missing or null values since we are only focused on the gender of the user which are male and female. Since gender carries rich and significant information concerning the male and female social activities. Then, we are checking if there are still any missing or null values and the output is 'False' meaning there are no missing values anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_label = data_text_label[data_text_label['gender'] != 'brand']\n",
    "data_text_label = data_text_label[data_text_label['gender'] != 'unknown']\n",
    "data_text_label = data_text_label[data_text_label['gender'] != 'XXX']\n",
    "data_text_label['gender'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Checking the shape of the dataset</b>\n",
    "\n",
    "After doing lots of data pre-processing techniques, we are just checking the shape of the dataset. Since we remove the 3 insignificant classes from 'gender' which are brand, unknown, and missing or null values. We remove the stopwords, duplicate records, and other insignificant values and noise in our data. The shape of our dataset is now (12709, 11) since there are 12709 records and 11 columns which are the gender and the 10 most significant extracted features because we are dealing with a text-based gender classifcation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12709, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_label['gender'].value_counts()\n",
    "data_text_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting values of 'gender' from string to int</b>\n",
    "\n",
    "In order to implement the machine learning models which are Naive Bayes and SVM from the dataset. The values of the data should be integer so that the model can process it. Since computer and machines cannot process strings but integers only. So we will convert 'Male' to 1 and 'Female' to 0 then store it in 'gender'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_gender(v):\n",
    "  if v == 'male':\n",
    "    return 1\n",
    "  elif v == 'female':\n",
    "    return 0\n",
    "\n",
    "data_text_label['gender'] = data_text_label.apply(lambda x: convert_gender(x['gender']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the gender column has now integer values instead of string. 0 which represents 'Female' and 1 which represents 'Male'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuxiliaryVerbsAndWords</th>\n",
       "      <th>AveCharWord</th>\n",
       "      <th>ConjunctionsAndWords</th>\n",
       "      <th>CountChar</th>\n",
       "      <th>InterjectionsAndWords</th>\n",
       "      <th>LengthOfWord</th>\n",
       "      <th>PrepositionsAndWords</th>\n",
       "      <th>ProWords</th>\n",
       "      <th>Ratio6WordsTotalWord</th>\n",
       "      <th>RationSpaChar</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AuxiliaryVerbsAndWords  AveCharWord  ConjunctionsAndWords  CountChar  \\\n",
       "0                0.000000     0.194444              0.071429         72   \n",
       "1                0.111111     0.272727              0.055556         66   \n",
       "2                0.000000     0.246154              0.062500         65   \n",
       "3                0.047619     0.250000              0.000000         84   \n",
       "4                0.000000     0.207547              0.000000         53   \n",
       "\n",
       "   InterjectionsAndWords  LengthOfWord  PrepositionsAndWords  ProWords  \\\n",
       "0               0.000000            14              0.285714  0.000000   \n",
       "1               0.000000            18              0.111111  0.222222   \n",
       "2               0.000000            16              0.062500  0.250000   \n",
       "3               0.047619            21              0.095238  0.047619   \n",
       "4               0.000000            11              0.272727  0.000000   \n",
       "\n",
       "   Ratio6WordsTotalWord  RationSpaChar  gender  \n",
       "0              0.428571       0.180556       1  \n",
       "1              0.166667       0.257576       1  \n",
       "2              0.125000       0.230769       1  \n",
       "3              0.238095       0.238095       1  \n",
       "4              0.363636       0.188679       0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_label.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementation of Gender Classification using SVM and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this gender classification task, we will be implementing two machine learning algorithms which are:\n",
    "1. <b>SVM (Support Vector Machine)</b> - is a supervised machine learning algorithm used for classification and regression problems. However, SVM is used most likely in classification problems. SVM works by plotting each data item as a point in n-dimensional space (where n is the number of features or independent variables that the dataset has) with the value of a particular coordinate which is the value of each feature. Then, perform classification by finding the hyper-plane that differentiates the two classes very well.\n",
    "2. <b>Naive Bayes</b> - is a powerful probabilistic machine learning algorithm that is used for classification. Naive Bayes works by using the Bayes’ Theorem which is predicting the probabilities for each class such as the probability of that given record or data point belongs to a particular class. The class or category with the highest probability is considered as the most likely class. This study will specifically implement Multinomial Naive Bayes, the only difference of it with Naive Bayes is that it is a multinomial distribution which requires integer feature counts, rather than some other distribution. This works well and is suitable for classification with discrete features, such as word counts in text which is one of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Dataset Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Splitting the dataset into training data and testing data</b>\n",
    "\n",
    "First step is of course we need to split the dataset into training data and testing data. In this case we set the test size as 25% making the training size 75% and adjusted the 'random_state' to 69 since it is the optimal value. \n",
    "1. <b>Training data</b> - is used to fit the machine learning model.\n",
    "2. <b>Testing data</b> - is used to evaluate the fit machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9531, 10)\n",
      "(3178, 10)\n",
      "(9531,)\n",
      "(3178,)\n",
      "Testing Data\n",
      "Male  Count:  1525\n",
      "Female Count:  1653\n",
      "Training Data\n",
      "Male  Count:  4611\n",
      "Female Count:  4920\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "male_count = 0\n",
    "female_count = 0\n",
    "\n",
    "for i in y_test:\n",
    "    if(i==1):\n",
    "        male_count+=1\n",
    "    else:\n",
    "        female_count+=1\n",
    "\n",
    "        \n",
    "        \n",
    "print(\"Testing Data\")\n",
    "print(\"Male  Count: \", male_count)\n",
    "print(\"Female Count: \", female_count)\n",
    "\n",
    "male_count = 0\n",
    "female_count = 0\n",
    "\n",
    "for i in y_train:\n",
    "    if(i==1):\n",
    "        male_count+=1\n",
    "    else:\n",
    "        female_count+=1\n",
    "\n",
    "        \n",
    "        \n",
    "print(\"Training Data\")\n",
    "print(\"Male  Count: \", male_count)\n",
    "print(\"Female Count: \", female_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataset into training data and testing data, we will now proceed in training the training data and fit it in the Support Vector Machine (SVM) model specifically in the Support Vector Classification (SVC). The parameters used in SVM are the following:\n",
    "1. <b>C</b> - is the regularization parameter. The strength of the regularization is inversely proportional to C. The value of C is set to its default value which is 1.0 since it is the optimal value in this case. \n",
    "2. <b>kernel</b> - is the kernel type to be used in the algorithm. The kernel that we used is 'linear' since the data can be linearly separable.\n",
    "\n",
    "After fitting the training data into the SVM model, we will now proceed to prediction of outcomes using the testing data. Then, we will get the obtained accuracy score from the SVM model that we created which is 56%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  55.72687224669603\n",
      "Accuracy: 0.557269\n",
      "Precision: 0.546094\n",
      "Recall: 0.458361\n",
      "F1 score: 0.498396\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma=\"auto\", class_weight = None)\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predictions_SVM)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predictions_SVM)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predictions_SVM)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1072</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>826</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1072  581\n",
       "1   826  699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predictions_SVM, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the gender classification task using Support Vector Machine (SVM), we will now implement it using Naive Bayes specifically Multinomial Naive Bayes since it works well on discrete features, such as word counts in text. The parameter used in Multinomial Naive Bayes is:\n",
    "1. <b>alpha</b> - is the smoothing parameter (0 for no smoothing). In this case, we used the default value of '1.0' since it is the optimal value. \n",
    "\n",
    "We will now create the Multinomial Naive Bayes model by training the training data and fit it in the Multinomial Naive Bayes model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the training data into the Multinomial Naive Bayes model, we will now proceed to prediction of outcomes using the testing data. Then, we will get the obtained accuracy score from the Multinomial Naive Bayes model that we created which is 55%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  55.12901195720579\n",
      "Accuracy: 0.551290\n",
      "Precision: 0.586087\n",
      "Recall: 0.220984\n",
      "F1 score: 0.320952\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy Score -> \", np.mean(predicted == y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predicted)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predicted)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predicted)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1415</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1188</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1415  238\n",
       "1  1188  337"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_units = ['SVM', 'Naive Bayes']\n",
    "\n",
    "y_units = [f1_score(predictions_SVM, y_test)*100, f1_score(predicted, y_test)*100]\n",
    "\n",
    "model_label = ['SVM', 'Naive Bayes']\n",
    "\n",
    "plt.bar(x_units, y_units, tick_label=model_label,\n",
    "        width=0.8, color=['red', 'blue'])\n",
    "\n",
    "plt.xlabel('Machine Learning Algorithm')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('SVM Vs. Naive Bayes')\n",
    "\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning(C, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  55.31780994336061\n",
      "Accuracy: 0.553178\n",
      "Precision: 0.541634\n",
      "Recall: 0.447869\n",
      "F1 score: 0.490309\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "SVM = svm.SVC(C=10.0, kernel='linear')\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predictions_SVM)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predictions_SVM)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predictions_SVM)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  0.552863436123348\n",
      "Accuracy: 0.552863\n",
      "Precision: 0.588737\n",
      "Recall: 0.226230\n",
      "F1 score: 0.326859\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=10.0).fit(X_train, y_train)\n",
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", np.mean(predicted == y_test))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predicted)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predicted)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predicted)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application of Pre-Processing Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More pre-processing techniques are applied for experimentation to analyze which will increase the accuracy and performance of each model since many machine learning algorithms perform better when features are on a similar scale and near to normally distributed. The pre-processing techniques performed in the two machine learning models which are Multinomial Naive Bayes and Support Vector Machine (SVM) are:\n",
    "1. <b>StandardScaler</b> - standardize features by removing the mean and scaling to unit variance\n",
    "2. <b>MinMaxScaler</b> - transform features by scaling each feature to a given range.\n",
    "3. <b>Normalizer</b> - normalize samples individually to unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  55.2863436123348\n",
      "Accuracy: 0.552863\n",
      "Precision: 0.537901\n",
      "Recall: 0.483934\n",
      "F1 score: 0.509493\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma=\"auto\", class_weight = None)\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predictions_SVM)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predictions_SVM)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predictions_SVM)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1019</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>787</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1019  634\n",
       "1   787  738"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test,predictions_SVM, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  56.167400881057276\n",
      "Accuracy: 0.561674\n",
      "Precision: 0.548105\n",
      "Recall: 0.493115\n",
      "F1 score: 0.519158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", np.mean(predicted == y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predicted)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predicted)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predicted)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1033</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>773</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1033  620\n",
       "1   773  752"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_units = ['SVM', 'Naive Bayes']\n",
    "\n",
    "y_units = [f1_score(predictions_SVM, y_test)*100, f1_score(predicted, y_test)*100]\n",
    "\n",
    "model_label = ['SVM', 'Naive Bayes']\n",
    "\n",
    "plt.bar(x_units, y_units, tick_label=model_label,\n",
    "        width=0.8, color=['red', 'blue'])\n",
    "\n",
    "plt.xlabel('Machine Learning Algorithm')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('SVM Vs. Naive Bayes')\n",
    "\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  55.66393958464443\n",
      "Accuracy: 0.556639\n",
      "Precision: 0.539456\n",
      "Recall: 0.520000\n",
      "F1 score: 0.529549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma=\"auto\", class_weight = None)\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predictions_SVM)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predictions_SVM)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predictions_SVM)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>976</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  976  677\n",
       "1  732  793"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predictions_SVM, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  53.146633102580246\n",
      "Accuracy: 0.531466\n",
      "Precision: 0.623288\n",
      "Recall: 0.059672\n",
      "F1 score: 0.108917\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", np.mean(predicted == y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predicted)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predicted)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predicted)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1598</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1434</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1\n",
       "0  1598  55\n",
       "1  1434  91"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_units = ['SVM', 'Naive Bayes']\n",
    "\n",
    "y_units = [f1_score(predictions_SVM, y_test)*100, f1_score(predicted, y_test)*100]\n",
    "\n",
    "model_label = ['SVM', 'Naive Bayes']\n",
    "\n",
    "plt.bar(x_units, y_units, tick_label=model_label,\n",
    "        width=0.8, color=['red', 'blue'])\n",
    "\n",
    "plt.xlabel('Machine Learning Algorithm')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('SVM Vs. Naive Bayes')\n",
    "\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizer with Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  53.901825047199495\n",
      "Accuracy: 0.539018\n",
      "Precision: 0.560000\n",
      "Recall: 0.183607\n",
      "F1 score: 0.276543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma=\"auto\", class_weight = None)\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predictions_SVM)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predictions_SVM)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predictions_SVM)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1433  220\n",
       "1  1245  280"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predictions_SVM, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  55.380742605412216\n",
      "Accuracy: 0.553807\n",
      "Precision: 0.529640\n",
      "Recall: 0.626885\n",
      "F1 score: 0.574174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_text_label[[\"AuxiliaryVerbsAndWords\", \"AveCharWord\", 'ConjunctionsAndWords', \"CountChar\", 'InterjectionsAndWords', \"LengthOfWord\", 'PrepositionsAndWords', 'ProWords', \"Ratio6WordsTotalWord\", \"RationSpaChar\"]]\n",
    "y = data_text_label[\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", np.mean(predicted == y_test)*100)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, predicted)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, predicted)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, predicted)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569</td>\n",
       "      <td>956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  804  849\n",
       "1  569  956"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = np.unique(y_test)\n",
    "cmtx =  confusion_matrix(y_test, predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cmtx, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_units = ['SVM', 'Naive Bayes']\n",
    "\n",
    "y_units = [f1_score(predictions_SVM, y_test)*100, f1_score(predicted, y_test)*100]\n",
    "\n",
    "model_label = ['SVM', 'Naive Bayes']\n",
    "\n",
    "plt.bar(x_units, y_units, tick_label=model_label,\n",
    "        width=0.8, color=['red', 'blue'])\n",
    "\n",
    "plt.xlabel('Machine Learning Algorithm')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('SVM Vs. Naive Bayes')\n",
    "\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
